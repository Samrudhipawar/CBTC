import requests
from bs4 import BeautifulSoup
import pandas as pd

class DataMiner:
    def __init__(self, base_url, selectors):
        self.base_url = base_url
        self.selectors = selectors

    def fetch_page(self, url):
        response = requests.get(url)
        if response.status_code == 200:
            return response.text
        else:
            print(f"Failed to retrieve page. Status code: {response.status_code}")
            return None

    def parse_page(self, html_content):
        soup = BeautifulSoup(html_content, 'html.parser')
        page_data = []
        for selector in self.selectors:
            elements = soup.select(selector['selector'])
            page_data.append([element.get_text(strip=True) for element in elements])
        return list(zip(*page_data))  # Transpose to match data rows with columns

    def store_data(self, data, filename):
        df = pd.DataFrame(data, columns=[selector['name'] for selector in self.selectors])
        df.to_csv(filename, index=False)
        print(f"Data stored in {filename}")

    def run(self, pages):
        all_data = []
        for page in pages:
            url = self.base_url + page
            print(f"Fetching {url}")
            html_content = self.fetch_page(url)
            if html_content:
                page_data = self.parse_page(html_content)
                all_data.extend(page_data)
        self.store_data(all_data, 'books_data.csv')

if __name__ == "__main__":
    # Example usage for the Books to Scrape website
    base_url = "http://books.toscrape.com/catalogue/"
    selectors = [
        {'name': 'Title', 'selector': 'h3 a'},  # Book title
        {'name': 'Price', 'selector': '.price_color'},  # Price
        {'name': 'Stock', 'selector': '.instock.availability'},  # Stock status
    ]
    pages = [f'page-{i}.html' for i in range(1, 3)]  # First two pages

    miner = DataMiner(base_url, selectors)
    miner.run(pages)
